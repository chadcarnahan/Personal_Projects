---
title: "Suicide Rates Data"
subtitle: ''
output:
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapse: yes
  pdf_document: default
---
```{r setup, include=FALSE, eval = TRUE}
knitr::opts_chunk$set(eval=TRUE,warning=FALSE)
library("png")
library(tidyverse)
library(formatR)
library("captioner")
figs <- captioner(prefix="Figure")
tbls <- captioner(prefix="Table")
```
#Load CSV and use summary function
```{r}
sData <- read.csv("~/Downloads/master.csv")

summary(sData)
```

#Gender Data
```{r}
#Filtering data for density because outliers are rare and skew density graph
densityData <- sData %>% filter(suicides.100k.pop < 30)
library(ggplot2)
# Density based on Gender, Males tend to commit suicide far more often, shifted data to not include extreme countries because trends in more common rates were more meaningful
suicideBySex<- sData %>% select(sex, suicides.100k.pop) %>% group_by(sex) %>% summarise(SuicidePer100k=mean(suicides.100k.pop))

ggplot(densityData, aes(x=suicides.100k.pop,fill=sex))+
  geom_density(alpha=0.4)

ggplot(suicideBySex, aes(x=sex, y=SuicidePer100k)) +
  geom_bar(stat="identity")
```


#Age Data
```{r}
#Density based on age groups, rates steadily climb as people get older 

suicideByAge <- sData %>% select(age, suicides.100k.pop) %>% group_by(age) %>% summarise(SuicidePer100k=mean(suicides.100k.pop))

generationAverages<-ggplot(data=suicideByAge, aes(x=age, y=SuicidePer100k)) +
  geom_bar(stat="identity")
generationAverages
```


#Country Data
```{r}
#Suicide by country, broken down into two sides, arranged by rate so countryone has lowe rate countries and countrytwo has higher rate countries 
suicideByCountry <- sData %>% select(country, suicides.100k.pop) %>% group_by(country) %>% summarise(SuicidePer100k=mean(suicides.100k.pop))

suicideByCountry <- arrange(suicideByCountry,SuicidePer100k)
countryOne <- suicideByCountry[1:50,]
countryTwo <- suicideByCountry[51:101,]


countryOneAverage<-ggplot(data=countryOne, aes(x=country, y=SuicidePer100k)) +
  geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90))
countryOneAverage

countryTwoAverage<-ggplot(data=countryTwo, aes(x=country, y=SuicidePer100k)) +
  geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90))
countryTwoAverage

```

#GDP Data
```{r}
#suicide by GDP, found some meaningful correlation rates are similar until 20 + that's when poorer countries start to be represented more
suicideByGDP <- sData %>% select(gdp_per_capita...., suicides.100k.pop) %>% group_by(gdp_per_capita....) %>% summarise(SuicidePer100k=mean(suicides.100k.pop))

ggplot(suicideByGDP, aes(x=SuicidePer100k, y=gdp_per_capita....)) +
  geom_point(size=.5, shape=1, colour = "blue",alpha=0.5)

```

#Population Data
```{r}
#suicide by population, this was completely random no correlation 
suicideByPop <- sData %>% select(population, suicides.100k.pop) %>% group_by(population) %>% summarise(SuicidePer100k=mean(suicides.100k.pop))

ggplot(suicideByPop, aes(x=SuicidePer100k, y=population)) +
  geom_point(size=1, shape=1, color = "red",alpha=0.4)

```

#HDI Data
```{r}
# Suicide based on countries with human development index, seperated into countries with 7.5 above/below then 6 above/below, went with 7.5 first to keep data balanced only represented 1/4 of data overall though. Neither categories showed any meaningful difference interestingly enough.
suicideByHDI <- sData %>%select(HDI.for.year,suicideRate = suicides.100k.pop) %>%drop_na() %>% group_by(HighScorer = HDI.for.year > .75, LowScorer = HDI.for.year < .6) 

ggplot(suicideByHDI, aes(x=suicideRate,fill=HighScorer))+
  geom_density(alpha=0.2)

ggplot(suicideByHDI, aes(x=suicideRate,fill=LowScorer))+
  geom_density(alpha=0.2)

```

#Year Data
```{r}
#Suicide based on year, went with plain bar graph no real insight gained 
suicideByYear <- sData %>% select(year, suicides.100k.pop) %>% group_by(year) %>% summarise(SuicidePer100k=mean(suicides.100k.pop))

yearBar<-ggplot(data=suicideByYear, aes(x=year, y=SuicidePer100k)) +
  geom_bar(stat="identity")

yearBar

```

#Normalize function
```{r}
normalize.Dataset <- function (data, range=1){
  data.norm <- data
  types <- sapply(data, is.numeric)
  for(i in 1:length(types)) {
    if (types[i]==TRUE){
      v <- data [,i]
      data.norm[,i] <- (v-min(v,na.rm=TRUE)) /(max(v,na.rm=TRUE)-min(v,na.rm=TRUE)) * range
    }
  }
  return(data.norm)
}

```

#Clustering
```{r}
summary(sData)
sData2 <- sData
library(dplyr)
library(cluster)
library(tidyverse)

#drop values that are redundant or not useful for the model 
drops <- c("country.year","HDI.for.year",'suicides_no',"gdp_for_year....","generation")
sData2 <- sData2[ , !(names(sData2) %in% drops)]
#Drop na data
sData2 <- na.omit(sData2)

#bin variables based on quartiles 
sData2$population<-cut(sData2$population, c(278,97498,430150,1486143,438025124))
sData2$suicides.100k.pop <- cut(sData2$suicides.100k.pop, c(0.00,0.92,5.99,16.62,224.97))
sData2$gdp_per_capita....<- cut(sData2$gdp_per_capita....,c(251,3447,9372,24874,126352))
sData2$year<-cut(sData2$year, c(1985,1995,2002,2008,2016))

#mutate all variables into numeric, drop NA values, normalize the dataset 
sData2 <- mutate_all(sData2, function(x) as.numeric(x))
sData2 <- na.omit(sData2)
sData2 <- normalize.Dataset(sData2)

clusters <- kmeans(sData2,centers=6,nstart=50)
library(factoextra)
p3 <- fviz_cluster(clusters,geom="point", sData2) + ggtitle('k = 8')
p3


```

#Naive Bayes Full Data Set
```{r}

#model without HDI
library(e1071)

#Cut sData2 into a train/test datasets, mutate train/test back into factor,
train <- sData2[1:18467,]
test <- sData2[18468:nrow(sData2),]
train <- mutate_all(train, function(x) as.factor(x))
test <- mutate_all(test, function(x) as.factor(x))

#create model using all values stored in sData2 
bayesModel <- naiveBayes(as.factor(suicides.100k.pop)~country + year+sex+age + population + gdp_per_capita...., 
                    data = train)


#predict using bayesmodel
pred.raw <- predict(bayesModel, test, type = "class")

#create confusion matrix based on how well it predicts suicide.100k.pop then calculate accuracy
confusion <- table(predict(bayesModel, test), 
      test$suicides.100k.pop, 
      dnn=c("prediction","truth"))

confusion
sum(diag(confusion)/nrow(test))
#60% accuracy not bad considering there is 4 potential options 


```

#Bayes With HDI
```{r}
#Model with HDI but a lot less rows
#reference sData(original dataset) to use with the new set used for Naive Bayes
bayesWithHDI <- sData
#Drop values that won't be used in the model
drops <- c("country.year",'suicides_no',"gdp_for_year....","generation")
bayesWithHDI <- bayesWithHDI[ , !(names(bayesWithHDI) %in% drops)]
#drop any NAs
bayesWithHDI <- na.omit(bayesWithHDI)

#Use this to bin continuuous values into categorical values, uses their quartiles as binning cuts/breaks
bayesWithHDI$population<-cut(bayesWithHDI$population, c(278,97498,430150,1486143,438025124))
bayesWithHDI$suicides.100k.pop <- cut(bayesWithHDI$suicides.100k.pop, c(0.00,0.92,5.99,16.62,224.97))
bayesWithHDI$gdp_per_capita....<- cut(bayesWithHDI$gdp_per_capita....,c(251,3447,9372,24874,126352))
bayesWithHDI$year<-cut(bayesWithHDI$year, c(1985,1995,2002,2008,2016))
bayesWithHDI$HDI.for.year<-cut(bayesWithHDI$HDI.for.year, c(.4830,.7130,.7790,.8550,.9440))

#mutate dataset into numeric, omit any nas again, normalize dataset
bayesWithHDI <- mutate_all(bayesWithHDI, function(x) as.numeric(x))
bayesWithHDI <- na.omit(bayesWithHDI)
bayesWithHDI <- normalize.Dataset(bayesWithHDI)

#make train/test datasets, revert back to factor 
trainHDI <- bayesWithHDI[1:6000,]
testHDI <- bayesWithHDI[6001:nrow(bayesWithHDI),]
trainHDI <- mutate_all(trainHDI, function(x) as.factor(x))
testHDI <- mutate_all(testHDI, function(x) as.factor(x))

#naive bayes model this time uses HDI
modelHDI <- naiveBayes(suicides.100k.pop~country+year+sex+age + population + gdp_per_capita.... + HDI.for.year, 
                    data = trainHDI)
#predict class labels for test dataset based on suicides.100k
pred.raw <- predict(modelHDI, testHDI, type = "class")
confusion <- table(predict(modelHDI, testHDI), 
      testHDI$suicides.100k.pop, 
      dnn=c("prediction","truth"))


confusion
#Find the accuracy of the model
sum(diag(confusion)/nrow(testHDI))

#only 53% lower than without, probably due to dropping a ton of data to use HDI 
```

